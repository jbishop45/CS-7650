{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbishop45/CS-7650/blob/project-2/project_2_NER_release_sp23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj21IyLWKoxk"
      },
      "source": [
        "# Licensing Information:  You are free to use or extend this project for\n",
        "# educational purposes provided that (1) you do not distribute or publish\n",
        "# solutions, (2) you retain this notice, and (3) you provide clear\n",
        "# attribution to The Georgia Institute of Technology, including a link to  https://aritter.github.io/CS-7650-sp22/\n",
        "\n",
        "# Attribution Information: This assignment was developed at The Georgia Institute of Technology\n",
        "# by Alan Ritter (alan.ritter@cc.gatech.edu)\n",
        "# Contributors: Xurui Zhang (Spring 2022)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U5ZbrBGbFes"
      },
      "source": [
        "# Project #2: Named Entity Recognition\n",
        "\n",
        "In this assignment, you will implement a bidirectional LSTM-CNN-CRF for sequence labeling, following [this paper by Xuezhe Ma and Ed Hovy](https://www.aclweb.org/anthology/P16-1101.pdf), on the CoNLL named entity recognition dataset.  Before starting the assignment, we recommend reading the Ma and Hovy paper.\n",
        "\n",
        "First, let's import some libraries and make sure the runtime has access to a GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJdlwzlqq0LI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "    print('and then re-execute this cell.')\n",
        "else:\n",
        "    print(gpu_info)\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6uw-MGrpEOb"
      },
      "source": [
        "## Download the Data\n",
        "\n",
        "Run the following code to download the English part of the CoNLL 2003 dataset, the evaluation script and pre-filtered GloVe embeddings we are providing for this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIzJzPU0p7e4"
      },
      "source": [
        "#CoNLL 2003 data\n",
        "!wget https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train\n",
        "!wget https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.testa\n",
        "!wget https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.testb\n",
        "!cat eng.train | awk '{print $1 \"\\t\" $4}' > train\n",
        "!cat eng.testa | awk '{print $1 \"\\t\" $4}' > dev\n",
        "!cat eng.testb | awk '{print $1 \"\\t\" $4}' > test\n",
        "\n",
        "#Evaluation Script\n",
        "!wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
        "\n",
        "#Pre-filtered GloVe embeddings\n",
        "!wget https://raw.githubusercontent.com/aritter/aritter.github.io/master/files/glove.840B.300d.conll_filtered.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv5CEGkGp3w1"
      },
      "source": [
        "## CoNLL Data Format\n",
        "\n",
        "Run the following cell to see a sample of the data in CoNLL format.  As you can see, each line in the file represents a word and its labeled named entity tag in BIO format.  A blank line is used to seperate sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbSvvi3Jtb4g"
      },
      "source": [
        "!head -n 20 train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM9F617sqbvc"
      },
      "source": [
        "## Reading in the Data\n",
        "\n",
        "Below we proivide a bit of code to read in data in the CoNLL format.  This also reads in the filtered GloVe embeddings, to save you some effort - we will discuss this more later."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in the training data\n",
        "def read_conll_format(filename):\n",
        "    (words, tags, currentSent, currentTags) = ([],[],['-START-'],['START'])\n",
        "    for line in open(filename).readlines():\n",
        "        line = line.strip()\n",
        "        #print(line)\n",
        "        if line == \"\":\n",
        "            currentSent.append('-END-')\n",
        "            currentTags.append('END')\n",
        "            words.append(currentSent)\n",
        "            tags.append(currentTags)\n",
        "            (currentSent, currentTags) = (['-START-'], ['START'])\n",
        "        else:\n",
        "            (word, tag) = line.split()\n",
        "            currentSent.append(word)\n",
        "            currentTags.append(tag)\n",
        "    return (words, tags)\n",
        "\n",
        "def sentences2char(sentences):\n",
        "    return [[['start'] + [c for c in w] + ['end'] for w in l] for l in sentences]\n",
        "\n",
        "\n",
        "(sentences_train, tags_train) = read_conll_format(\"train\")\n",
        "(sentences_dev, tags_dev)     = read_conll_format(\"dev\")\n",
        "\n",
        "print(sentences_train[2])\n",
        "print(tags_train[2])\n",
        "\n",
        "sentencesChar = sentences2char(sentences_train)\n",
        "\n",
        "print(sentencesChar[2])"
      ],
      "metadata": {
        "id": "w026Wa_EzmJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read GloVe embeddings.\n",
        "def read_GloVe(filename):\n",
        "    embeddings = {}\n",
        "    for line in open(filename).readlines():\n",
        "        #print(line)\n",
        "        fields = line.strip().split(\" \")\n",
        "        word = fields[0]\n",
        "        embeddings[word] = [float(x) for x in fields[1:]]\n",
        "    return embeddings\n",
        "\n",
        "GloVe = read_GloVe(\"glove.840B.300d.conll_filtered.txt\")\n",
        "\n",
        "print(GloVe[\"the\"])\n",
        "print(\"dimension of glove embedding:\", len(GloVe[\"the\"]))"
      ],
      "metadata": {
        "id": "f6I4mPL8v6dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zeg1gSiqyLD"
      },
      "source": [
        "## Mapping Tokens to Indices\n",
        "\n",
        "As in the last project, we will need to convert words in the dataset to numeric indices, so they can be presented as input to a neural network.  Code to handle this for you with sample usage is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99p1PPX5qlPQ"
      },
      "source": [
        "#Create mappings between tokens and indices.\n",
        "\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "#Will need this later to remove 50% of words that only appear once in the training data from the vocabulary (and don't have GloVe embeddings).\n",
        "wordCounts = Counter([w for l in sentences_train for w in l])\n",
        "charCounts = Counter([c for l in sentences_train for w in l for c in w])\n",
        "singletons = set([w for (w,c) in wordCounts.items() if c == 1 and not w in GloVe.keys()])\n",
        "charSingletons = set([w for (w,c) in charCounts.items() if c == 1])\n",
        "\n",
        "#Build dictionaries to map from words, characters to indices and vice versa.\n",
        "#Save first two words in the vocabulary for padding and \"UNK\" token.\n",
        "word2i = {w:i+2 for i,w in enumerate(set([w for l in sentences_train for w in l] + list(GloVe.keys())))}\n",
        "char2i = {w:i+2 for i,w in enumerate(set([c for l in sentencesChar for w in l for c in w]))}\n",
        "i2word = {i:w for w,i in word2i.items()}\n",
        "i2char = {i:w for w,i in char2i.items()}\n",
        "\n",
        "vocab_size = max(word2i.values()) + 1\n",
        "char_vocab_size = max(char2i.values()) + 1\n",
        "\n",
        "#Tag dictionaries.\n",
        "tag2i = {w:i for i,w in enumerate(set([t for l in tags_train for t in l]))}\n",
        "i2tag = {i:t for t,i in tag2i.items()}\n",
        "\n",
        "#When training, randomly replace singletons with UNK tokens sometimes to simulate situation at test time.\n",
        "def getDictionaryRandomUnk(w, dictionary, train=False):\n",
        "    if train and (w in singletons and random.random() > 0.5):\n",
        "        return 1\n",
        "    else:\n",
        "        return dictionary.get(w, 1)\n",
        "\n",
        "#Map a list of sentences from words to indices.\n",
        "def sentences2indices(words, dictionary, train=False):\n",
        "    #1.0 => UNK\n",
        "    return [[getDictionaryRandomUnk(w,dictionary, train=train) for w in l] for l in words]\n",
        "\n",
        "#Map a list of sentences containing to indices (character indices)\n",
        "def sentences2indicesChar(chars, dictionary):\n",
        "    #1.0 => UNK\n",
        "    return [[[dictionary.get(c,1) for c in w] for w in l] for l in chars]\n",
        "\n",
        "#Indices\n",
        "X       = sentences2indices(sentences_train, word2i, train=True)\n",
        "X_char  = sentences2indicesChar(sentencesChar, char2i)\n",
        "Y       = sentences2indices(tags_train, tag2i)\n",
        "\n",
        "print(\"vocab size:\", vocab_size)\n",
        "print(\"char vocab size:\", char_vocab_size)\n",
        "print()\n",
        "\n",
        "print(\"index of word 'the':\", word2i[\"the\"])\n",
        "print(\"word of index 253:\", i2word[253])\n",
        "print()\n",
        "\n",
        "#Print out some examples of what the dev inputs will look like\n",
        "for i in range(10):\n",
        "    print(\" \".join([i2word.get(w,'UNK') for w in X[i]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHGxN6QCr3GY"
      },
      "source": [
        "## Padding and Batching\n",
        "\n",
        "In this assignment, you should train your models using minibatched SGD, rather than using a batch size of 1 as we did in the previous project.  When presenting multiple sentences to the network at the same time, we will need to pad them to be of the same length. We use [torch.nn.utils.rnn.pad_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) to do so.\n",
        "\n",
        "Below we provide some code to prepare batches of data to present to the network.  pad the sequence so that all the sequences have the same length.\n",
        "\n",
        "**Side Note:** PyTorch includes utilities in [`torch.utils.data`](https://pytorch.org/docs/stable/data.html) to help with padding, batching, shuffling and some other things, but for this assignment we will do everything from scratch to help you see exactly how this works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHuYuoSYmgiC"
      },
      "source": [
        "#Pad inputs to max sequence length (for batching)\n",
        "def prepare_input(X_list):\n",
        "    X_padded = torch.nn.utils.rnn.pad_sequence([torch.as_tensor(l) for l in X_list], batch_first=True).type(torch.LongTensor) # padding the sequences with 0\n",
        "    X_mask   = torch.nn.utils.rnn.pad_sequence([torch.as_tensor([1.0] * len(l)) for l in X_list], batch_first=True).type(torch.FloatTensor) # consisting of 0 and 1, 0 for padded positions, 1 for non-padded positions\n",
        "    return (X_padded, X_mask)\n",
        "\n",
        "#Maximum word length (for character representations)\n",
        "MAX_CLEN=32\n",
        "\n",
        "def prepare_input_char(X_list):\n",
        "    MAX_SLEN = max([len(l) for l in X_list])\n",
        "    X_padded  = [l + [[]]*(MAX_SLEN-len(l))  for l in X_list]\n",
        "    X_padded  = [[w[0:MAX_CLEN] for w in l] for l in X_padded]\n",
        "    X_padded  = [[w + [1]*(MAX_CLEN-len(w)) for w in l] for l in X_padded]\n",
        "    return torch.as_tensor(X_padded).type(torch.LongTensor)\n",
        "\n",
        "#Pad outputs using one-hot encoding\n",
        "def prepare_output_onehot(Y_list, NUM_TAGS=max(tag2i.values())+1):\n",
        "    Y_onehot = [torch.zeros(len(l), NUM_TAGS) for l in Y_list]\n",
        "    for i in range(len(Y_list)):\n",
        "        for j in range(len(Y_list[i])):\n",
        "            Y_onehot[i][j,Y_list[i][j]] = 1.0\n",
        "    Y_padded = torch.nn.utils.rnn.pad_sequence(Y_onehot, batch_first=True).type(torch.FloatTensor)\n",
        "    return Y_padded\n",
        "\n",
        "print(\"max slen:\", max([len(x) for x in X_char]))  #Max sequence length in the training data is 39.\n",
        "\n",
        "(X_padded, X_mask) = prepare_input(X)\n",
        "X_padded_char      = prepare_input_char(X_char)\n",
        "Y_onehot           = prepare_output_onehot(Y)\n",
        "\n",
        "print(\"X_padded:\", X_padded.shape)\n",
        "print(\"X_mask:\", X_mask.shape)\n",
        "print(\"X_padded_char:\", X_padded_char.shape)\n",
        "print(\"Y_onehot:\", Y_onehot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = prepare_input(X[0:5])[0]\n",
        "print(\"input: \", input.shape)"
      ],
      "metadata": {
        "id": "Wx9xfpEAftj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSynCsJzu717"
      },
      "source": [
        "## **Your code starts here:** Basic LSTM Tagger (10 points)\n",
        "\n",
        "OK, now you should have everything you need to get started.\n",
        "\n",
        "Recall that your goal is to to implement the BiLSTM-CNN-CRF, as described in [(Ma and Hovy, 2016)](https://www.aclweb.org/anthology/P16-1101.pdf).  This is a relatively complex network with various components.  Below we provide starter code to break down your implementation into increasingly complex versions of the final model, starting with a Basic LSTM tagger.  This way you can be confident that each part is working correctly before incrementally increasing the complexity of your implementation.  This is generally a good approach to take when implementing complex models, since buggy PyTorch code is often partially working, but produces worse results than a correct implementation, so it's hard to know whether added complexities are helping or hurting.  Also, if you aren't able to match published results it's hard to know which component of your model has the problem (or even whether or not it is a problem in the published result!)\n",
        "\n",
        "Fill in the functions marked as `TODO` in the code block below.  If everything is working correctly, you should be able to achieve an **F1 score of 0.87 on the dev set and 0.83 on the test set (with GloVe embeddings)**. You are required to initialize word embeddings with GloVe later, but you can randomly initialize the word embeddings in the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl5EsH2WgxLh"
      },
      "source": [
        "class BasicLSTMtagger(nn.Module):\n",
        "    def __init__(self, DIM_EMB=10, DIM_HID=10):\n",
        "        super(BasicLSTMtagger, self).__init__()\n",
        "        NUM_TAGS = max(tag2i.values())+1\n",
        "        (self.DIM_EMB, self.NUM_TAGS) = (DIM_EMB, NUM_TAGS)\n",
        "\n",
        "        #TODO: initialize parameters - embedding layer, nn.LSTM, nn.Linear and nn.LogSoftmax\n",
        "        bidirectional=False\n",
        "        in_features = DIM_HID*2 if bidirectional else DIM_HID\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=100000,embedding_dim=DIM_EMB)\n",
        "        self.LSTM = nn.LSTM(input_size=DIM_EMB, hidden_size=DIM_HID, batch_first=True, bidirectional=bidirectional)\n",
        "        self.L = nn.Linear(in_features=in_features, out_features=NUM_TAGS)\n",
        "        self.lsm = nn.LogSoftmax()\n",
        "\n",
        "        self.h_n = None\n",
        "        self.c_n = None\n",
        "\n",
        "    def forward(self, X, train=False):\n",
        "        # X is X_padded from prepare_input()\n",
        "        #TODO: Implement the forward computation.\n",
        "        emb = self.emb(X)\n",
        "        #print('emb: ' + str(emb.shape))\n",
        "        if self.h_n is not None and self.c_n is not None:\n",
        "          con, (self.h_n, self.c_n) = self.LSTM(emb,self.h_n,self.c_n)\n",
        "        else:\n",
        "          con, (self.h_n, self.c_n) = self.LSTM(emb)\n",
        "        #print('con: ' + str(con.shape))\n",
        "        fla = self.L(con)\n",
        "        #print('fla: ' + str(fla.shape))\n",
        "        log = self.lsm(fla)\n",
        "        return log\n",
        "        #return torch.randn((X.shape[0], X.shape[1], self.NUM_TAGS))  #Random baseline.\n",
        "\n",
        "    def init_glove(self, GloVe):\n",
        "        #TODO: initialize word embeddings using GloVe (you can skip this part in your first version, if you want, see instructions below).\n",
        "        pass\n",
        "\n",
        "    def inference(self, sentences):\n",
        "        X       = prepare_input(sentences2indices(sentences, word2i))[0].cuda()\n",
        "        pred = self.forward(X).argmax(dim=2)\n",
        "        return [[i2tag[pred[i,j].item()] for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
        "\n",
        "    def print_predictions(self, words, tags):\n",
        "        Y_pred = self.inference(words)\n",
        "        for i in range(len(words)):\n",
        "            print(\"----------------------------\")\n",
        "            print(\" \".join([f\"{words[i][j]}/{Y_pred[i][j]}/{tags[i][j]}\" for j in range(len(words[i]))]))\n",
        "            print(\"Predicted:\\t\", Y_pred[i])\n",
        "            print(\"Gold:\\t\\t\", tags[i])\n",
        "\n",
        "    def write_predictions(self, sentences, outFile):\n",
        "        fOut = open(outFile, 'w')\n",
        "        for s in sentences:\n",
        "            y = self.inference([s])[0]\n",
        "            #print(\"\\n\".join(y[1:len(y)-1]))\n",
        "            fOut.write(\"\\n\".join(y[1:len(y)-1]))  #Skip start and end tokens\n",
        "            fOut.write(\"\\n\\n\")\n",
        "\n",
        "#The following code will initialize a model and test that your forward computation runs without errors.\n",
        "lstm_test   = BasicLSTMtagger(DIM_HID=7, DIM_EMB=300)\n",
        "lstm_output = lstm_test.forward(prepare_input(X[0:5])[0]) # torch.Size([5, 32])\n",
        "Y_onehot    = prepare_output_onehot(Y[0:5])\n",
        "\n",
        "#Check the shape of the lstm_output and one-hot label tensors.\n",
        "print(\"lstm output shape:\", lstm_output.shape)\n",
        "print(\"Y onehot shape:\", Y_onehot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99ZVaS50gyGY"
      },
      "source": [
        "#Read in the data\n",
        "\n",
        "(sentences_dev, tags_dev)     = read_conll_format('dev')\n",
        "(sentences_train, tags_train) = read_conll_format('train')\n",
        "(sentences_test, tags_test)   = read_conll_format('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Xbox3alX0b"
      },
      "source": [
        "# Train your Model (10 points)\n",
        "\n",
        "Next, implement the function below to train your basic BiLSTM tagger.  See [torch.nn.lstm](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).  Make sure to save your predictions on the test set (`test_pred_lstm.txt`) for submission to GradeScope. Feel free to change number of epochs, optimizer, learning rate and batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvJyLIjR6HjT"
      },
      "source": [
        "#Training\n",
        "\n",
        "from random import sample\n",
        "import tqdm\n",
        "import os\n",
        "import subprocess\n",
        "import random\n",
        "\n",
        "def shuffle_sentences(sentences, tags):\n",
        "    shuffled_sentences = []\n",
        "    shuffled_tags      = []\n",
        "    indices = list(range(len(sentences)))\n",
        "    random.shuffle(indices)\n",
        "    for i in indices:\n",
        "        shuffled_sentences.append(sentences[i])\n",
        "        shuffled_tags.append(tags[i])\n",
        "    return (shuffled_sentences, shuffled_tags)\n",
        "\n",
        "nEpochs = 10\n",
        "\n",
        "def train_basic_lstm(sentences, tags, lstm):\n",
        "  #optimizer = optim.Adadelta(lstm.parameters(), lr=0.1)\n",
        "  #TODO: initialize optimizer\n",
        "    optimizer = optim.SGD(lstm.parameters(),lr=0.1)\n",
        "    loss_function = nn.NLLLoss()\n",
        "\n",
        "    batchSize = 50\n",
        "\n",
        "    for epoch in range(nEpochs):\n",
        "        totalLoss = 0.0\n",
        "\n",
        "        (sentences_shuffled, tags_shuffled) = shuffle_sentences(sentences, tags)\n",
        "\n",
        "        print('sentences shuffled: ' + str(len(sentences_shuffled)))\n",
        "        print('tags shuffled: ' + str(len(tags_shuffled)))\n",
        "        for batch in tqdm.notebook.tqdm(range(0, len(sentences), batchSize), leave=False):\n",
        "            lstm.zero_grad()\n",
        "            #TODO: Implement gradient update.\n",
        "              # https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "\n",
        "            X_batch = sentences2indices(sentences_shuffled[batch], word2i, train=True) #train=True\n",
        "            print(len(X_batch))\n",
        "            Y_batch = sentences2indices(tags_shuffled[batch], tag2i)\n",
        "            print(len(Y_batch))\n",
        "            X_batch_prepared = prepare_input(X_batch)[0].cuda()\n",
        "            print('X_batch_prepared: ' + str(X_batch_prepared.shape))\n",
        "            Y_batch_onehot   = prepare_output_onehot(Y_batch).cuda() #.argmax(dim=1)\n",
        "            print('Y_onehot: ' + str(Y_batch_onehot.shape))\n",
        "            pred = lstm.forward(X_batch_prepared) #.argmax(dim=1)\n",
        "            print('pred: ' + str(pred.shape))\n",
        "            \n",
        "            #loss = loss_function(pred, Y_batch_onehot)\n",
        "            loss = torch.einsum(torch.neg(torch.log(pred)).dot(Y_batch_onehot))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            totalLoss += loss\n",
        "\n",
        "        print(f\"loss on epoch {epoch} = {totalLoss}\")\n",
        "        lstm.write_predictions(sentences_dev, 'dev_pred')   #Performance on dev set\n",
        "        print('conlleval:')\n",
        "        print(subprocess.Popen('paste dev dev_pred | perl conlleval.pl -d \"\\t\"', shell=True, stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].decode('UTF-8'))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            s = sample(range(len(sentences_dev)), 5)\n",
        "            lstm.print_predictions([sentences_dev[i] for i in s], [tags_dev[i] for i in s])\n",
        "\n",
        "lstm = BasicLSTMtagger(DIM_HID=500, DIM_EMB=300).cuda()\n",
        "train_basic_lstm(sentences_train, tags_train, lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om69KEe_KKxs"
      },
      "source": [
        "#Evaluation on test data\n",
        "lstm.write_predictions(sentences_test, 'test_pred_lstm.txt')\n",
        "!wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
        "!paste test test_pred_lstm.txt | perl conlleval.pl -d \"\\t\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvyY4qzvuBzK"
      },
      "source": [
        "## Initialization with GloVe Embeddings (5 points)\n",
        "\n",
        "If you haven't already, implement the `init_glove()` method in `BasicLSTMtagger` above.\n",
        "\n",
        "Rather than initializing word embeddings randomly, it is common to use learned word embeddings (GloVe or Word2Vec), as discussed in lecture.  To make this simpler, we have already pre-filtered [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings to only contain words in the vocabulary of the CoNLL NER dataset, and loaded them into a dictionary (`GloVe`) at the beginning of this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdJSNuMKuN8J"
      },
      "source": [
        "## Character Embeddings (10 points)\n",
        "\n",
        "Now that you have your basic LSTM tagger working, the next step is to add a convolutional network that computes word embeddings from character representations of words.  See Figure 2 and Figure 3 in the [Ma and Hovy](https://www.aclweb.org/anthology/P16-1101.pdf) paper.  We have provided code in `sentences2input_tensors` to convert sentences into lists of word and character indices.  See also [nn.Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html) and [MaxPool1d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html).\n",
        "\n",
        "Hint: The nn.Conv1d accepts input size $(N, C_{in}, L_{in})$, but we have input size $(N, \\text{SLEN}, \\text{CLEN}, \\text{EMB_DIM})$. We can reshape and [permute](https://pytorch.org/docs/stable/generated/torch.permute.html) our input to satisfy the nn.Conv1d, and recover the dimensions later.\n",
        "\n",
        "Make sure to save your predictions on the test set, for submission to GradeScope.  You should be able to achieve **90 F1 / 85 F1 on the dev/test sets**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwiKOvSs7Xgx"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CharLSTMtagger(BasicLSTMtagger): \n",
        "    def __init__(self, DIM_EMB=10, DIM_CHAR_EMB=30, DIM_HID=10):\n",
        "        super(CharLSTMtagger, self).__init__(DIM_EMB=DIM_EMB, DIM_HID=DIM_HID)\n",
        "        NUM_TAGS = max(tag2i.values())+1\n",
        "\n",
        "        (self.DIM_EMB, self.NUM_TAGS) = (DIM_EMB, NUM_TAGS)\n",
        "        #TODO: Initialize parameters.\n",
        "\n",
        "    def forward(self, X, X_char, train=False):\n",
        "        #TODO: Implement the forward computation.\n",
        "        return torch.randn((X.shape[0], X.shape[1], self.NUM_TAGS))  #Random baseline.\n",
        "\n",
        "    def sentences2input_tensors(self, sentences):\n",
        "        (X, X_mask)   = prepare_input(sentences2indices(sentences, word2i))\n",
        "        X_char        = prepare_input_char(sentences2indicesChar(sentences, char2i))\n",
        "        return (X, X_mask, X_char)\n",
        "\n",
        "    def inference(self, sentences):\n",
        "        (X, X_mask, X_char) = self.sentences2input_tensors(sentences)\n",
        "        pred = self.forward(X.cuda(), X_char.cuda()).argmax(dim=2)\n",
        "        return [[i2tag[pred[i,j].item()] for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
        "\n",
        "    def print_predictions(self, words, tags):\n",
        "        Y_pred = self.inference(words)\n",
        "        for i in range(len(words)):\n",
        "            print(\"----------------------------\")\n",
        "            print(\" \".join([f\"{words[i][j]}/{Y_pred[i][j]}/{tags[i][j]}\" for j in range(len(words[i]))]))\n",
        "            print(\"Predicted:\\t\", Y_pred[i])\n",
        "            print(\"Gold:\\t\\t\", tags[i])\n",
        "\n",
        "char_lstm_test = CharLSTMtagger(DIM_HID=7, DIM_EMB=300)\n",
        "lstm_output    = char_lstm_test.forward(prepare_input(X[0:5])[0], prepare_input_char(X_char[0:5]))\n",
        "Y_onehot       = prepare_output_onehot(Y[0:5])\n",
        "\n",
        "print(\"lstm output shape:\", lstm_output.shape)\n",
        "print(\"Y onehot shape:\", Y_onehot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FmQqNC_CB7Z"
      },
      "source": [
        "#Training LSTM w/ character embeddings. Feel free to change number of epochs, optimizer, learning rate and batch size.\n",
        "\n",
        "nEpochs = 10\n",
        "\n",
        "def train_char_lstm(sentences, tags, lstm):\n",
        "  #optimizer = optim.Adadelta(lstm.parameters(), lr=0.1)\n",
        "  #TODO: initialize optimizer\n",
        "\n",
        "    batchSize = 50\n",
        "\n",
        "    for epoch in range(nEpochs):\n",
        "        totalLoss = 0.0\n",
        "\n",
        "        (sentences_shuffled, tags_shuffled) = shuffle_sentences(sentences, tags)\n",
        "        for batch in tqdm.notebook.tqdm(range(0, len(sentences), batchSize), leave=False):\n",
        "            lstm.zero_grad()\n",
        "            #TODO: Gradient update\n",
        "\n",
        "\n",
        "        print(f\"loss on epoch {epoch} = {totalLoss}\")\n",
        "        lstm.write_predictions(sentences_dev, 'dev_pred')   #Performance on dev set\n",
        "        print('conlleval:')\n",
        "        print(subprocess.Popen('paste dev dev_pred | perl conlleval.pl -d \"\\t\"', shell=True, stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].decode('UTF-8'))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            s = sample(range(len(sentences_dev)), 5)\n",
        "            lstm.print_predictions([sentences_dev[i] for i in s], [tags_dev[i] for i in s])\n",
        "\n",
        "char_lstm = CharLSTMtagger(DIM_HID=500, DIM_EMB=300).cuda()\n",
        "train_char_lstm(sentences_train, tags_train, char_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9NaMeuCnS3r"
      },
      "source": [
        "#Evaluation on test set\n",
        "char_lstm.write_predictions(sentences_test, 'test_pred_cnn_lstm.txt')\n",
        "!wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
        "!paste test test_pred_cnn_lstm.txt | perl conlleval.pl -d \"\\t\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4P4Ycm1CT2"
      },
      "source": [
        "## Conditional Random Fields (15 points)\n",
        "\n",
        "Now we are ready to add a CRF layer to the `CharacterLSTMTagger`.  To train the model, implement `conditional_log_likelihood`, using the score (unnormalized log probability) of the gold sequence, in addition to the partition function, $Z(X)$, which is computed using the forward algorithm.  Then, you can simply use Pytorch's automatic differentiation to compute gradients by running backpropagation through the computation graph of the dynamic program (this should be very simple, so long as you are able to correctly implement the forward algorithm using a computation graph that is supported by PyTorch).  This approach to computing gradients for CRFs is discussed in Section 7.5.3 of the [Eisenstein Book](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
        "\n",
        "You will also need to implement the Viterbi algorithm for inference during decoding.\n",
        "\n",
        "After including CRF training and Viterbi decoding, you should be getting about **92 F1 / 88 F1 on the dev and test set**, respectively.\n",
        "\n",
        "**IMPORTANT:** Note that training will be substantially slower this time - depending on the efficiency of your implementation, it could take about 5 minutes per epoch (e.g. 50 minutes for 10 iterations).  It is recommended to start out training on a single batch of data (and testing on this same batch), so that you can quickly debug, making sure your model can memorize the labels on a single batch, and then optimize your code.  Once you are fairly confident your code is working properly, then you can train using the full dataset.  We have provided a (commented out) line of code to switch between training on a single batch and the full dataset below.\n",
        "\n",
        "**Hint #1:** While debugging your implementation of the Forward algorithm it is helpful to look at the loss during training.  The loss should never be less than zero (the log-likelihood should always be negative).\n",
        "\n",
        "**Hint #2:** To sum log-probabilities in a numerically stable way at the end of the Forward algorithm, you will want to use [`torch.logsumexp`](https://pytorch.org/docs/stable/generated/torch.logsumexp.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKBomV-L6SmZ"
      },
      "source": [
        "#For F.max_pool1d()\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTM_CRFtagger(CharLSTMtagger):\n",
        "    def __init__(self, DIM_EMB=10, DIM_CHAR_EMB=30, DIM_HID=10, N_TAGS=max(tag2i.values())+1):\n",
        "        super(LSTM_CRFtagger, self).__init__(DIM_EMB=DIM_EMB, DIM_HID=DIM_HID, DIM_CHAR_EMB=DIM_CHAR_EMB)\n",
        "\n",
        "        #TODO: Initialize parameters.\n",
        "\n",
        "        self.transitionWeights = nn.Parameter(torch.zeros((N_TAGS, N_TAGS), requires_grad=True))\n",
        "        nn.init.normal_(self.transitionWeights)\n",
        "\n",
        "    def gold_score(self, lstm_scores, Y):\n",
        "        #TODO: compute score of gold sequence Y (unnormalized conditional log-probability)\n",
        "        return 0\n",
        "\n",
        "    #Forward algorithm for a single sentence\n",
        "    #Efficiency will eventually be important here.  We recommend you start by \n",
        "    #training on a single batch and make sure your code can memorize the \n",
        "    #training data.  Then you can go back and re-write the inner loop using \n",
        "    #tensor operations to speed things up.\n",
        "    def forward_algorithm(self, lstm_scores, sLen):\n",
        "        #TODO: implement forward algorithm.\n",
        "        return 0\n",
        "\n",
        "    def conditional_log_likelihood(self, sentences, tags, train=True):\n",
        "        #Todo: compute conditional log likelihood of Y (use forward_algorithm and gold_score)\n",
        "        return 0\n",
        "\n",
        "    def viterbi(self, lstm_scores, sLen):\n",
        "        #TODO: Implement Viterbi algorithm, soring backpointers to recover the argmax sequence.  Returns the argmax sequence in addition to its unnormalized conditional log-likelihood.\n",
        "        return (torch.as_tensor([random.randint(0,lstm_scores.shape[1]-1) for x in range(sLen)]), 0)\n",
        "\n",
        "    #Computes Viterbi sequences on a batch of data.\n",
        "    def viterbi_batch(self, sentences):\n",
        "        viterbiSeqs = []\n",
        "        (X, X_mask, X_char) = self.sentences2input_tensors(sentences)\n",
        "        lstm_scores = self.forward(X.cuda(), X_char.cuda())\n",
        "        for s in range(len(sentences)):\n",
        "            (viterbiSeq, ll) = self.viterbi(lstm_scores[s], len(sentences[s]))\n",
        "            viterbiSeqs.append(viterbiSeq)\n",
        "        return viterbiSeqs\n",
        "\n",
        "    def forward(self, X, X_char, train=False):\n",
        "        #TODO: Implement the forward computation.\n",
        "        return torch.randn((X.shape[0], X.shape[1], self.NUM_TAGS))  #Random baseline.\n",
        "\n",
        "    def print_predictions(self, words, tags):\n",
        "        Y_pred = self.inference(words)\n",
        "        for i in range(len(words)):\n",
        "            print(\"----------------------------\")\n",
        "            print(\" \".join([f\"{words[i][j]}/{Y_pred[i][j]}/{tags[i][j]}\" for j in range(len(words[i]))]))\n",
        "            print(\"Predicted:\\t\", [Y_pred[i][j] for j in range(len(words[i]))])\n",
        "            print(\"Gold:\\t\\t\", tags[i])\n",
        "\n",
        "    #Need to use Viterbi this time.\n",
        "    def inference(self, sentences, viterbi=True):\n",
        "        pred = self.viterbi_batch(sentences)\n",
        "        return [[i2tag[pred[i][j].item()] for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
        "\n",
        "lstm_crf = LSTM_CRFtagger(DIM_EMB=300).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjcSzfi27uOM"
      },
      "source": [
        "print(lstm_crf.conditional_log_likelihood(sentences_dev[0:3], tags_dev[0:3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in7f4nDWc6dd"
      },
      "source": [
        "#CharLSTM-CRF Training. Feel free to change number of epochs, optimizer, learning rate and batch size.\n",
        "import tqdm\n",
        "import os\n",
        "import subprocess\n",
        "import random\n",
        "\n",
        "nEpochs = 10\n",
        "\n",
        "#Get CoNLL evaluation script\n",
        "os.system('wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl')\n",
        "\n",
        "def train_crf_lstm(sentences, tags, lstm):\n",
        "    #optimizer = optim.Adadelta(lstm.parameters(), lr=1.0)\n",
        "    #TODO: initialize optimizer\n",
        "\n",
        "    batchSize = 50\n",
        "\n",
        "    for epoch in range(nEpochs):\n",
        "        totalLoss = 0.0\n",
        "        lstm.train()\n",
        "\n",
        "        #Shuffle the sentences\n",
        "        (sentences_shuffled, tags_shuffled) = shuffle_sentences(sentences, tags)\n",
        "        for batch in tqdm.notebook.tqdm(range(0, len(sentences), batchSize), leave=False):\n",
        "            lstm.zero_grad()\n",
        "            #TODO: take gradient step on a batch of data.\n",
        "\n",
        "        print(f\"loss on epoch {epoch} = {totalLoss}\")\n",
        "        lstm.write_predictions(sentences_dev, 'dev_pred')   #Performance on dev set\n",
        "        print('conlleval:')\n",
        "        print(subprocess.Popen('paste dev dev_pred | perl conlleval.pl -d \"\\t\"', shell=True, stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].decode('UTF-8'))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            lstm.eval()\n",
        "            s = random.sample(range(50), 5)\n",
        "            lstm.print_predictions([sentences_train[i] for i in s], [tags_train[i] for i in s])   #Print predictions on train data (useful for debugging)\n",
        "\n",
        "crf_lstm = LSTM_CRFtagger(DIM_HID=500, DIM_EMB=300, DIM_CHAR_EMB=30).cuda()\n",
        "train_crf_lstm(sentences_train, tags_train, crf_lstm)             #Train on the full dataset\n",
        "#train_crf_lstm(sentences_train[0:50], tags_train[0:50])          #Train only the first batch (use this during development/debugging)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4omMnnmjJPRD"
      },
      "source": [
        "crf_lstm.eval()\n",
        "crf_lstm.write_predictions(sentences_test, 'test_pred_cnn_lstm_crf.txt')\n",
        "!wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
        "!paste test test_pred_cnn_lstm_crf.txt | perl conlleval.pl -d \"\\t\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkjfQ1kRyAKa"
      },
      "source": [
        "## Gradescope\n",
        "\n",
        "Gradescope allows you to add multiple files to your submission. Please submit this notebook along with the test set prediction:\n",
        "* test_pred_lstm.txt\n",
        "* test_pred_cnn_lstm.txt\n",
        "* test_pred_cnn_lstm_crf.txt\n",
        "* NER_release.ipynb\n",
        "\n",
        "To download this notebook, go to `File > Download.ipynb`. You can download the predictions from Colab by clicking the folder icon on the left and finding them under Files. \n",
        "\n",
        "Please make sure that you name the files as specified above. You will be able to see the test set accuracy for your predictions. However, the final score will be assigned later based on accuracy and implementation. \n",
        "\n",
        "When submitting the .ipynb notebook, please make sure that all the cells run when executed in order starting from a fresh session. If the code doesn't take too long to run, you can re-run everything with `Runtime -> Restart and run all`\n",
        "\n",
        "You can submit multiple times before the deadline and choose the submission which you want to be graded by going to `Submission History` on gradescope.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhlUIp_OydGz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}